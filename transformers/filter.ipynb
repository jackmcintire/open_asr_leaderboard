{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87b4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "\n",
    "# add open_asr_leaderboard project root to sys.path\n",
    "ROOT = pathlib.Path().resolve().parent          # one level up from transformers\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b040435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from normalizer.data_utils import normalizer as data_utils_normalizer\n",
    "normalizer = data_utils_normalizer\n",
    "\n",
    "\n",
    "def create_filtered_file(special_words_name, input_file_name):\n",
    "    \"\"\"\n",
    "    Filters a JSONL file based on a list of special words, adds the matched\n",
    "    words to each JSON object, and creates a new filtered file.\n",
    "    \"\"\"\n",
    "    INPUT_FILE_PATH = f\"results/{input_file_name}.jsonl\"\n",
    "    base, ext = os.path.splitext(INPUT_FILE_PATH)\n",
    "    if \"jmci-aispeak-v1\" in input_file_name:\n",
    "        output_file_path = f\"{base}-{special_words_name}.jsonl\".replace(\"jmci-aispeak-v1\", special_words_name.replace(\"_\", \"-\"))\n",
    "    else:\n",
    "        output_file_path = f\"{base}-{special_words_name}.jsonl\".replace(\"aquavoice-cleaned\", special_words_name.replace(\"_\", \"-\"))\n",
    "\n",
    "    # 1. Load and normalize special words\n",
    "    SPECIAL_WORDS_PATH = f\"{special_words_name}.csv\"\n",
    "    print(f\"Loading special words from {SPECIAL_WORDS_PATH}...\")\n",
    "    try:\n",
    "        with open(SPECIAL_WORDS_PATH, 'r', encoding='utf-8') as f:\n",
    "            special_words = [line.strip().rstrip(',') for line in f if line.strip()]\n",
    "            normalized_special_words = {normalizer(word) for word in special_words}\n",
    "            print(f\"Loaded and normalized {len(normalized_special_words)} special words.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Special words file not found at '{SPECIAL_WORDS_PATH}'\")\n",
    "        return\n",
    "\n",
    "    # 2. Filter the input file and add matched words\n",
    "    print(f\"Filtering {INPUT_FILE_PATH}...\")\n",
    "    lines_written = 0\n",
    "    try:\n",
    "        with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "             open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "            for line in infile:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    text = data.get('text', '')\n",
    "\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    normalized_text = normalizer(text)\n",
    "                    words_in_text = set(normalized_text.split())\n",
    "\n",
    "                    # Find which special words are in the text\n",
    "                    matched_words = words_in_text.intersection(normalized_special_words)\n",
    "\n",
    "                    if matched_words:\n",
    "                        # Add the matched words to the JSON object\n",
    "                        data['matched_special_words'] = sorted(list(matched_words))\n",
    "                        # Write the modified object to the output file\n",
    "                        outfile.write(json.dumps(data) + '\\n')\n",
    "                        lines_written += 1\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping malformed JSON line: {line.strip()}\")\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{INPUT_FILE_PATH}'\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nFiltering complete.\")\n",
    "    print(f\" - Total lines written: {lines_written}\")\n",
    "    print(f\" - Filtered file saved to: {os.path.abspath(output_file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading special words from global_dict_10.csv...\n",
      "Loaded and normalized 10 special words.\n",
      "Filtering results/MODEL_openai-whisper-large_DATASET_jmci-aispeak-v1_default_test.jsonl...\n",
      "\n",
      "Filtering complete.\n",
      " - Total lines written: 1278\n",
      " - Filtered file saved to: /lambda/nfs/jtm/open_asr_leaderboard/transformers/results/MODEL_openai-whisper-large_DATASET_global-dict-10_default_test-global_dict_10.jsonl\n",
      "Loading special words from global_dict_50.csv...\n",
      "Loaded and normalized 48 special words.\n",
      "Filtering results/MODEL_openai-whisper-large_DATASET_jmci-aispeak-v1_default_test.jsonl...\n",
      "\n",
      "Filtering complete.\n",
      " - Total lines written: 3652\n",
      " - Filtered file saved to: /lambda/nfs/jtm/open_asr_leaderboard/transformers/results/MODEL_openai-whisper-large_DATASET_global-dict-50_default_test-global_dict_50.jsonl\n",
      "Loading special words from global_dict_500.csv...\n",
      "Loaded and normalized 569 special words.\n",
      "Filtering results/MODEL_openai-whisper-large_DATASET_jmci-aispeak-v1_default_test.jsonl...\n",
      "\n",
      "Filtering complete.\n",
      " - Total lines written: 9198\n",
      " - Filtered file saved to: /lambda/nfs/jtm/open_asr_leaderboard/transformers/results/MODEL_openai-whisper-large_DATASET_global-dict-500_default_test-global_dict_500.jsonl\n"
     ]
    }
   ],
   "source": [
    "create_filtered_file(\"global_dict_10\", \"MODEL_openai-gpt-4o-transcribe_DATASET_jmci-aispeak-v1_default_test\")\n",
    "create_filtered_file(\"global_dict_50\", \"MODEL_openai-gpt-4o-transcribe_DATASET_jmci-aispeak-v1_default_test\")\n",
    "create_filtered_file(\"global_dict_500\", \"MODEL_openai-gpt-4o-transcribe_DATASET_jmci-aispeak-v1_default_test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
